{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 13 Exercise - Transformers\n",
    "\n",
    "In this notebook, we will explore the creation of a Transformer Network for English to French translation.  Note that **Transformers are resource intensive and hard to train.** You will want to run these notebooks on a machine equipped with a GPU or on [Google Colab](http://colab.research.google.com).\n",
    "\n",
    "To begin, let's import a corpus of paired English and French text.  Additionally, we'll tokenize the words (i.e. create a dictionary for each vocabulary associating every word with an integer index).  There is no need to modify this cell, but have a look at what is contained in fr_to_ix (for example) and in enlines.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import math\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if torch.cuda.is_available():  \n",
    "    device = \"cuda:0\" \n",
    "else:  \n",
    "    device = \"cpu\" \n",
    "\n",
    "with open('./french.txt', encoding=\"utf-8\") as file:\n",
    "    frvocab = file.read().lower()\n",
    "    frvocab = ''.join([i if ord(i) < 128 else ' ' for i in frvocab])\n",
    "    frlines = frvocab.split('\\n')\n",
    "frlines = [re.sub(r'[^\\w\\s\\']','',i).split() for i in frlines]\n",
    "frvocab = set(re.sub(r'[^\\w\\s\\']','',frvocab).replace('\\n',' ').split(' '))\n",
    "\n",
    "with open('./english.txt', encoding=\"utf-8\") as file:\n",
    "    envocab = file.read().lower()\n",
    "    envocab = ''.join([i if ord(i) < 128 else '' for i in envocab])\n",
    "    enlines = envocab.split('\\n')\n",
    "enlines = [re.sub(r'[^\\w\\s]','',i).split() for i in enlines]\n",
    "envocab = set(re.sub(r'[^\\w\\s]','',envocab).replace('\\n',' ').strip().split(' '))\n",
    "envocab.add('<pad>')\n",
    "envocab.add('<start>')\n",
    "envocab.add('<eos>')\n",
    "frvocab.add('<pad>')\n",
    "frvocab.add('<start>')\n",
    "frvocab.add('<eos>')\n",
    "fr_to_ix = {word: i for i, word in enumerate(frvocab)}\n",
    "en_to_ix = {word: i for i, word in enumerate(envocab)}\n",
    "ix_to_fr = {fr_to_ix[word]:word for word in frvocab}\n",
    "ix_to_en = {en_to_ix[word]:word for word in envocab}\n",
    "enmax = 0\n",
    "frmax = 16\n",
    "\n",
    "for i,w in enumerate(enlines):\n",
    "    temp = len(w)\n",
    "    if temp > enmax:\n",
    "        enmax = temp\n",
    "\n",
    "for i,w in enumerate(frlines):\n",
    "    temp = len(w)\n",
    "    if temp > frmax:\n",
    "        frmax = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll create a handful of helper functions that do things like\n",
    " - Tokenize an english string, run it through the transformer producing predictions, then convert back to a french string\n",
    " - Compare predicted and target output\n",
    " - Mask a string\n",
    " - Load paired english/french sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    # Read in an english string\n",
    "    line = re.sub(r'[^\\w\\s]','',sentence).split()\n",
    "    # tokenize/pad for consistent sequence length\n",
    "    line = F.pad(torch.tensor([en_to_ix[w.lower()] for w in line]),(0,enmax-len(line)),value = en_to_ix['<pad>']).unsqueeze(0).to(device)\n",
    "    # Create an array to hold the French sentence\n",
    "    target = torch.Tensor(1,frmax-1)\n",
    "    target = target.new_full((1,frmax-1),fr_to_ix['<pad>']).long().to(device)\n",
    "    # Start sentence with a <start> character\n",
    "    target[0,0] = fr_to_ix['<start>']\n",
    "    \n",
    "    src,trg = mask(line,target)\n",
    "    encoding = model.encode(line,src)\n",
    "    K,V = model.create_dec_KV(encoding)\n",
    "    for i in range(1,frmax-1):\n",
    "        test2 = model.decode(target,K,V,src,trg)\n",
    "        lastout = test2[0,i-1].argmax()\n",
    "        if lastout.item() == fr_to_ix['<eos>']:\n",
    "            break\n",
    "        target[0,i] = lastout\n",
    "        src,trg = mask(line,target)\n",
    "    translation = test2.argmax(2).squeeze(0)\n",
    "    translation_string = ''\n",
    "    for w in translation:\n",
    "        if ix_to_fr[w.item()] == '<eos>':\n",
    "            break\n",
    "        translation_string += ix_to_fr[w.item()] + ' '\n",
    "    return translation_string.strip()\n",
    "\n",
    "def compareoutput(preds,targetlist,loc=None):\n",
    "    # Compare model predictions with true translation\n",
    "    if loc is None:\n",
    "        loc = np.random.randint(len(preds))\n",
    "    predstr = ''\n",
    "    labelstr = ''\n",
    "    for i in range(len(preds[loc][0])):\n",
    "        if ix_to_fr[targetlist[loc][i+1].item()] == '<eos>':\n",
    "            break\n",
    "        predstr += ' '+ ix_to_fr[preds[loc][0][i].item()]\n",
    "        labelstr += ' ' + ix_to_fr[targetlist[loc][i+1].item()]\n",
    "    print(\"\\tOutput:\", predstr)\n",
    "    print(\"\\tTarget:\",labelstr)\n",
    "    \n",
    "class PositionalEncoder(nn.Module):\n",
    "    # Create a positional encoding generator\n",
    "    def __init__(self, d_model, max_seq_len = 58):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # create constant 'pe' matrix with values dependant on \n",
    "        # pos and i\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        for pos in range(max_seq_len):\n",
    "            for i in range(0, d_model, 2):\n",
    "                pe[pos, i] = \\\n",
    "                math.sin(pos / (10000 ** ((2 * i)/d_model)))\n",
    "            for i in range(1,d_model,2):\n",
    "                pe[pos, i] = \\\n",
    "                math.cos(pos / (10000 ** ((2 * i)/d_model)))\n",
    "                \n",
    "        pe = pe\n",
    "        self.register_buffer('pe', pe)\n",
    " \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # make embeddings relatively larger\n",
    "        x = x * math.sqrt(self.d_model)\n",
    "        #add constant to embedding\n",
    "        seq_len = x.size(1)\n",
    "        x = x + Variable(self.pe[:seq_len], \\\n",
    "        requires_grad=False).to(device)\n",
    "        return x\n",
    "\n",
    "def mask(input_seq,target_seq):\n",
    "    input_msk = (input_seq != en_to_ix['<pad>']).unsqueeze(1)\n",
    "    target_msk = (target_seq != fr_to_ix['<pad>']).unsqueeze(1)\n",
    "    size = target_seq.size(1) # get seq_len for matrix\n",
    "    nopeak_mask = np.triu(np.ones((1, size, size)),k=1)\n",
    "    nopeak_mask = Variable(torch.from_numpy(nopeak_mask).to(device) == 0)\n",
    "    target_msk = target_msk & nopeak_mask\n",
    "    return input_msk,target_msk\n",
    "\n",
    "class custdata(Dataset):\n",
    "    # Create a custom dataset object to serve up paired english and french lines\n",
    "    def __init__(self,enlines,frlines):\n",
    "        self.data_len = len(enlines) \n",
    "        self.data = [F.pad(torch.tensor([en_to_ix[w] for w in line]),(0,enmax-len(line)),value = en_to_ix['<pad>']).to(device) for line in enlines]\n",
    "        self.labels = []\n",
    "        for line in frlines:\n",
    "            line = ['<start>',*line,'<eos>']\n",
    "            self.labels.append(F.pad(torch.tensor([fr_to_ix[w] for w in line]),(0,frmax-len(line)),value = fr_to_ix['<pad>']).to(device))\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.data[i],self.labels[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention\n",
    "The first task is to code a self-attention mechanism, which corresponds to implementing Eq. 1 in Vaswani.\n",
    "#### http://jalammar.github.io/illustrated-transformer/ is a great reference for most of the programming in this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class self_attention(nn.Module):\n",
    "    def __init__(self,dim,enc_dim,dropout = .1):\n",
    "        super().__init__()\n",
    "        self.wq = nn.Linear(dim, enc_dim) #### TODO#### WEIGHTS FOR Q, INPUT DIM = DIM, OUTPUT DIM = ENC_DIM\n",
    "        self.wk = nn.Linear(dim,enc_dim) #### TODO#### WEIGHTS FOR K, INPUT DIM = DIM, OUTPUT DIM = ENC_DIM\n",
    "        self.wv = nn.Linear(dim,enc_dim) #### TODO#### WEIGHTS FOR V, INPUT DIM = DIM, OUTPUT DIM = ENC_DIM\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.scaler = np.sqrt(enc_dim)\n",
    "    \n",
    "    def QKV(self,x):\n",
    "        Q = self.wq(x)  #### TODO#### CALCULATE Q\n",
    "        K = self.wk(x)  #### TODO#### CALCULATE K\n",
    "        V = self.wv(x)  #### TODO#### CALCULATE V\n",
    "        return Q,K,V\n",
    "    \n",
    "    def score(self,Q,K,V,mask):\n",
    "        # scores are the stuff that goes inside the softmax\n",
    "        scores = Q@K.T.permute(2,0,1)/self.scaler ### TODO ### CALCULATE THE SCORES. !!!DONT TOUCH THE PERMUTE!!!\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        scores = self.dropout(F.softmax(scores,-1)) \n",
    "        return scores@V ### TODO ### FINISH CALCULATING SELF ATTENTION\n",
    "    \n",
    "    def forward(self,x,mask=None):\n",
    "        Q,K,V = self.QKV(x)\n",
    "        return self.score(Q,K,V,mask)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to produce the \"special\" attention mechanism that takes keys and values from the encoder, but queries from the decoder.  This is very similar to the self-attention mechanism, except that there should be two inputs.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encdec_attention(nn.Module):\n",
    "    def __init__(self,dim,dropout = .1):\n",
    "        super().__init__()\n",
    "        self.wq = nn.Linear(dim,dim) \n",
    "        self.wk = nn.Linear(dim,dim) \n",
    "        self.wv = nn.Linear(dim,dim)\n",
    "        self.scaler = np.sqrt(dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def Q(self,x):\n",
    "        return self.wq(x)\n",
    "    \n",
    "    def score(self,Q,K,V,mask):\n",
    "        # scores are the stuff that goes inside the softmax\n",
    "        scores = Q@K.T.permute(2,0,1)/self.scaler ### TODO ### CALCULATE THE SCORES. !!!DONT TOUCH THE PERMUTE!!!\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        scores = self.dropout(F.softmax(scores,-1)) \n",
    "        return scores@V ### TODO ### FINISH CALCULATING SELF ATTENTION\n",
    "    \n",
    "    def forward(self,x,K,V,mask):\n",
    "        # DB Note: I'm not sure that this signature is right.  Seems like we should be taking x from the\n",
    "        # decoder, as well as another argument (call it y?) from the encoder, then producing K,V,Q internally,\n",
    "        # just like in the self-attention scheme.  Otherwise, how are wk and wv being used here?  it looks like \n",
    "        # these parameters have been shifted over to the Transformer module's create_dec_KV method.\n",
    "        Q = self.Q(x)\n",
    "        out = self.score(Q,K,V,mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder and Decoder\n",
    "With the attention mechanisms coded, now we need to create encoder and decoder models. These correspond to the things inside the boxes in Figure 1 of Vaswani.  \n",
    "#### Fill in the forward passes of the encoder and decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self,dim,enc_dim,vocab_size,dropout=.1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.enc_dim = enc_dim\n",
    "        self.residual = nn.Linear(dim,enc_dim)\n",
    "        \n",
    "        self.attention = self_attention(dim,enc_dim,dropout)\n",
    "        self.norm1 = nn.LayerNorm(enc_dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(enc_dim,enc_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(enc_dim)\n",
    "    \n",
    "    def forward(self,x,mask):  #### TODO #### SET UP FORWARD PASS OF ENCODER\n",
    "        z = self.attention(x,mask)\n",
    "        if self.dim != self.enc_dim: ### DONT TOUCH, THIS IS TO HELP WITH THE RESIDUAL CONNECTION ###\n",
    "            x = self.residual(x)\n",
    "        z = self.norm1(x+z)\n",
    "        y = self.linear(z)\n",
    "        return self.norm2(z+y)\n",
    "     \n",
    "        \n",
    "class decoder(nn.Module):\n",
    "    def __init__(self,dim,input_size,vocab_size,dropout=.1):\n",
    "        super().__init__()\n",
    "        self.attention = self_attention(input_size,dim,dropout)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.EDattention = encdec_attention(dim,dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(dim,dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.norm3 = nn.LayerNorm(dim)\n",
    "    \n",
    "    def forward(self,x,k,v,enc_mask,dec_mask):#### TODO #### SET UP FORWARD PASS OF DECODER\n",
    "        z = self.attention(x,dec_mask)\n",
    "        z = self.norm1(x+z)\n",
    "        y  = self.EDattention(z,k,v,enc_mask)\n",
    "        z = self.norm2(y+z)\n",
    "        w = self.linear(z)\n",
    "        return self.norm3(w+z)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer\n",
    "\n",
    "Build the transformer itself by hooking together encoders and decoders.  Note the word embedding layers that we are going to learn.  \n",
    "\n",
    "#### Add encoders and decoders to transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer(nn.Module):\n",
    "    def __init__(self,dim,encoder_dim,enc_vocab_size,dec_vocab_size,input_size):\n",
    "        super().__init__()\n",
    "        self.embedding1 = nn.Embedding(enc_vocab_size,dim)\n",
    "        self.embedding2 = nn.Embedding(dec_vocab_size,encoder_dim)\n",
    "        \n",
    "        self.pe1 = PositionalEncoder(dim,enmax)\n",
    "        self.pe2 = PositionalEncoder(encoder_dim,frmax)\n",
    "        self.encoders = []\n",
    "        self.decoders = []\n",
    "        \n",
    "        \n",
    "        for _ in range(3):\n",
    "            self.encoders.append( encoder(dim,encoder_dim,enc_vocab_size)) \n",
    "            self.decoders.append( decoder(encoder_dim,encoder_dim,dec_vocab_size))\n",
    "       \n",
    "        \n",
    "\n",
    "       \n",
    "        self.decoders = nn.ModuleList(self.decoders)\n",
    "        self.encoders = nn.ModuleList(self.encoders)\n",
    "    \n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(encoder_dim,dec_vocab_size),\n",
    "            nn.LogSoftmax(2)\n",
    "        )\n",
    "        \n",
    "        self.k = nn.Linear(encoder_dim,encoder_dim)\n",
    "        self.v = nn.Linear(encoder_dim,encoder_dim)\n",
    "        \n",
    "    def create_dec_KV(self,z):\n",
    "        K = self.k(z)\n",
    "        V = self.v(z)\n",
    "        return K,V\n",
    "    \n",
    "    def encode(self,x,src):\n",
    "        x = self.embedding1(x)\n",
    "        x = self.pe1(x)\n",
    "        for layer in self.encoders:\n",
    "            x = layer(x,src)\n",
    "        return x\n",
    "    \n",
    "    def decode(self,y,K,V,src,trg):\n",
    "        y = self.embedding2(y)\n",
    "        y = self.pe2(y)\n",
    "        for layer in self.decoders:\n",
    "            y = layer(y,K,V,src,trg)\n",
    "        return self.final(y)\n",
    "    \n",
    "    def forward(self,x,y,src,trg):\n",
    "        \n",
    "        x = self.encode(x,src)\n",
    "        K,V = self.create_dec_KV(x)\n",
    "        y = self.decode(y,K,V,src,trg)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the network.\n",
    "\n",
    "##### This will be slow to train and require a lot of resources. You can reduce the batch_size to lower the vram requirement, you can reduce \n",
    "##### the run time by lowering number_of_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformer(enmax,enmax,len(envocab),len(frvocab),frmax)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01,weight_decay=.001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor=.1,patience=2,threshold=1,min_lr=.0001)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "NUMBER_OF_LINES = 20000\n",
    "\n",
    "train = custdata(enlines[:NUMBER_OF_LINES],frlines[:NUMBER_OF_LINES])\n",
    "trainloader = torch.utils.data.DataLoader(dataset=train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val = custdata(enlines[NUMBER_OF_LINES:NUMBER_OF_LINES+1000],frlines[NUMBER_OF_LINES:NUMBER_OF_LINES+1000])\n",
    "valloader = torch.utils.data.DataLoader(dataset=val, batch_size=1, shuffle=True, drop_last=False)\n",
    "test = custdata(enlines[NUMBER_OF_LINES+1000:NUMBER_OF_LINES+2000],frlines[NUMBER_OF_LINES+1000:NUMBER_OF_LINES+2000])\n",
    "testloader = torch.utils.data.DataLoader(dataset=test, batch_size=1, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  loss: 168.8343631029129\n",
      "\tOutput:  <eos> <eos> <eos> <eos>\n",
      "\tTarget:  tout le monde vieillit\n",
      "Epoch: 2  loss: 79.20183649659157\n",
      "\tOutput:  je suis suis <eos> <eos> <eos>\n",
      "\tTarget:  tu ne peux pas me battre\n",
      "Epoch: 3  loss: 71.25045189261436\n",
      "\tOutput:  il pas <eos>\n",
      "\tTarget:  tesvous press s\n",
      "Epoch: 4  loss: 67.83122724294662\n",
      "\tOutput:  il me pas <eos>\n",
      "\tTarget:  vous tes fort courageux\n",
      "Epoch: 5  loss: 66.88067355751991\n",
      "\tOutput:  je <eos>\n",
      "\tTarget:  t'aije surpris\n",
      "Epoch: 6  loss: 66.24614191055298\n",
      "\tOutput:  nous <eos> tes\n",
      "\tTarget:  aimezvous les robots\n",
      "Epoch: 7  loss: 65.36426910758018\n",
      "\tOutput:  il est pas <eos>\n",
      "\tTarget:  tu es fort contrari\n",
      "Epoch: 8  loss: 65.22309783101082\n",
      "\tOutput:  nous vous est <eos> <eos>\n",
      "\tTarget:  qui vous a envoy ici\n",
      "Epoch: 9  loss: 65.0730519592762\n",
      "\tOutput:  nous <eos> t\n",
      "\tTarget:  astu un peigne\n",
      "Epoch: 10  loss: 64.97127723693848\n",
      "\tOutput:  nous est <eos>\n",
      "\tTarget:  y vastu galement\n",
      "Epoch: 11  loss: 64.86201924085617\n",
      "\tOutput:  nous <eos>\n",
      "\tTarget:  aimestu wagner\n",
      "Epoch: 12  loss: 64.73894801735878\n",
      "\tOutput:  nous tes pas pas <eos>\n",
      "\tTarget:  vous tes tr s craintif\n",
      "Epoch: 13  loss: 64.55637100338936\n",
      "\tOutput:  nous tes t t <eos>\n",
      "\tTarget:  vous tes le plus vieux\n",
      "Epoch: 14  loss: 64.43650287389755\n",
      "\tOutput:  nous <eos> le\n",
      "\tTarget:  vastu t'en servir\n",
      "Epoch: 15  loss: 64.25888615846634\n",
      "\tOutput:  nous tes le <eos>\n",
      "\tTarget:  vous devriez partir maintenant\n",
      "Epoch: 16  loss: 64.14759454131126\n",
      "\tOutput:  nous le t <eos>\n",
      "\tTarget:  tu es surmen e\n",
      "Epoch: 17  loss: 63.982304990291595\n",
      "\tOutput:  nous tes <eos> <eos>\n",
      "\tTarget:  votre femme est ici\n",
      "Epoch: 18  loss: 63.81739276647568\n",
      "\tOutput:  nous est t\n",
      "\tTarget:  peuxtu le prendre\n",
      "Epoch: 19  loss: 63.699381202459335\n",
      "\tOutput:  nous tes me\n",
      "\tTarget:  vous me tuez\n",
      "Epoch: 20  loss: 63.56039181351662\n",
      "\tOutput:  nous es est <eos>\n",
      "\tTarget:  tu peux entrer maintenant\n",
      "Epoch: 21  loss: 63.39823633432388\n",
      "\tOutput:  il es t <eos> le\n",
      "\tTarget:  tu es nouveau de retour\n",
      "Epoch: 22  loss: 63.232101142406464\n",
      "\tOutput:  nous t\n",
      "\tTarget:  l'avezvous invit\n",
      "Epoch: 23  loss: 63.17061701416969\n",
      "\tOutput:  il es pas <eos> le <eos>\n",
      "\tTarget:  tu n'as pas de c ur\n",
      "Epoch: 24  loss: 63.01535513997078\n",
      "\tOutput:  nous est t\n",
      "\tTarget:  qui la faute\n",
      "Epoch: 25  loss: 62.86067974567413\n",
      "\tOutput:  nous de\n",
      "\tTarget:  aimezvous wagner\n",
      "Epoch: 26  loss: 62.72768986225128\n",
      "\tOutput:  il ne pas <eos> s\n",
      "\tTarget:  ne marche pas si vite\n",
      "Epoch: 27  loss: 62.596122056245804\n",
      "\tOutput:  nous es t <eos> le\n",
      "\tTarget:  tu ferais mieux de partir\n",
      "Epoch: 28  loss: 62.45560458302498\n",
      "\tOutput:  nous est <eos>\n",
      "\tTarget:  pourquoi estu seule\n",
      "Epoch: 29  loss: 62.35304868221283\n",
      "\tOutput:  nous est <eos>\n",
      "\tTarget:  cela t'atil plu\n",
      "Epoch: 30  loss: 62.23315677046776\n",
      "\tOutput:  nous es un <eos>\n",
      "\tTarget:  tu es fort timide\n",
      "Epoch: 31  loss: 62.06675785779953\n",
      "\tOutput:  nous tes t pas\n",
      "\tTarget:  vous tes cern e\n",
      "Epoch: 32  loss: 61.92032581567764\n",
      "\tOutput:  nous es d t\n",
      "\tTarget:  tu es l'a n\n",
      "Epoch: 33  loss: 61.822583109140396\n",
      "\tOutput:  nous t\n",
      "\tTarget:  estu courageux\n",
      "Epoch: 34  loss: 61.677159786224365\n",
      "\tOutput:  c'est un <eos>\n",
      "\tTarget:  sontelles content es\n",
      "Epoch: 35  loss: 61.525780975818634\n",
      "\tOutput:  nous tes pas\n",
      "\tTarget:  vous tes cern\n",
      "Epoch: 36  loss: 61.373735666275024\n",
      "\tOutput:  nous <eos>\n",
      "\tTarget:  habitaientils ici\n",
      "Epoch: 37  loss: 61.230390787124634\n",
      "\tOutput:  nous t s\n",
      "\tTarget:  estu int ress\n",
      "Epoch: 38  loss: 61.1179955303669\n",
      "\tOutput:  nous tes pas s es le\n",
      "\tTarget:  vous tes tr s dr le\n",
      "Epoch: 39  loss: 60.951100796461105\n",
      "\tOutput:  nous un <eos>\n",
      "\tTarget:  astu bien dormi\n",
      "Epoch: 40  loss: 60.81761768460274\n",
      "\tOutput:  nous es un\n",
      "\tTarget:  tu es courageuse\n",
      "Epoch: 41  loss: 60.69634836912155\n",
      "\tOutput:  il tes me s <eos>\n",
      "\tTarget:  vous ne r ussirez pas\n",
      "Epoch: 42  loss: 60.56527501344681\n",
      "\tOutput:  nous tes un pas <eos>\n",
      "\tTarget:  vous avez l'air europ enne\n",
      "Epoch: 43  loss: 60.45973852276802\n",
      "\tOutput:  nous le\n",
      "\tTarget:  revenez demain\n",
      "Epoch: 44  loss: 60.31419360637665\n",
      "\tOutput:  il t <eos> le\n",
      "\tTarget:  l'aije dit de travers\n",
      "Epoch: 45  loss: 60.19545954465866\n",
      "\tOutput:  nous tes pas s es le\n",
      "\tTarget:  vous tes tr s dr les\n",
      "Epoch: 46  loss: 60.07085162401199\n",
      "\tOutput:  nous tes <eos>\n",
      "\tTarget:  vous lisez trop\n",
      "Epoch: 47  loss: 59.96558168530464\n",
      "\tOutput:  nous es un t <eos>\n",
      "\tTarget:  tu es le plus vieux\n",
      "Epoch: 48  loss: 59.850706070661545\n",
      "\tOutput:  nous tes pas s\n",
      "\tTarget:  vous tes un prisonnier\n",
      "Epoch: 49  loss: 59.76408076286316\n",
      "\tOutput:  nous es le <eos> <eos> s\n",
      "\tTarget:  tu aimes bien tout le monde\n",
      "Epoch: 50  loss: 59.658283054828644\n",
      "\tOutput:  nous t t\n",
      "\tTarget:  estu en vacances\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for j,(context, target) in enumerate(trainloader):\n",
    "        trg_input = target[:,:-1]\n",
    "        outmask = target[:,1:]!= fr_to_ix['<pad>']\n",
    "        targets = target[:,1:].contiguous().view(-1)\n",
    "        src,trg = mask(context,trg_input)\n",
    "        \n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        context = context.to(device)\n",
    "        trg_input = trg_input.to(device)\n",
    "        \n",
    "        output = model(context,trg_input,src,trg)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output.view(output.shape[0]*output.shape[1],-1),targets)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    scheduler.step(total_loss)\n",
    "    print('Epoch:', i+1,' loss:', total_loss)\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    preds = []\n",
    "    targetlist = []\n",
    "    for j,(context, target) in enumerate(valloader):\n",
    "            trg_input = target[:,:-1]\n",
    "            targets = target.contiguous().view(-1)\n",
    "            targetlist.append(targets)\n",
    "            src,trg = mask(context,trg_input)\n",
    "            output = model(context,trg_input,src,trg)\n",
    "            pred = F.softmax(output,2).argmax(2)\n",
    "            preds.append(pred)\n",
    "            break\n",
    "    compareoutput(preds,targetlist,loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test your translator\n",
    "\n",
    "##### Unless you speak french you're going have to check it with google translate https://translate.google.com/\n",
    "##### I found it started doing alright once the loss got below 10 but this might take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tom me le t s'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'how are you'\n",
    "translate(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test it on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average # of words correct 0.3398334054834055\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "scores = []\n",
    "preds = []\n",
    "targetlist = []\n",
    "for j,(context, target) in enumerate(testloader):\n",
    "        trg_input = target[:,:-1]\n",
    "        targets = target[:,1:].contiguous().view(-1)\n",
    "        targetlist.append(targets)\n",
    "        src,trg = mask(context,trg_input)\n",
    "        output = model(context,trg_input,src,trg)\n",
    "        pred = F.softmax(output,2).argmax(2)\n",
    "        preds.append(pred)\n",
    "        correct = sum(pred[0][targets!=fr_to_ix['<pad>']]==targets[targets!=fr_to_ix['<pad>']]).item()/len(targets[targets!=fr_to_ix['<pad>']])\n",
    "        scores.append(correct)\n",
    "plt.plot(scores)\n",
    "print('Average # of words correct',np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTI-HEAD ATTENTION (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class multi_attention(nn.Module):\n",
    "    def __init__(self,dim,encoder_dim,dropout = .1):\n",
    "        super().__init__()\n",
    "        \n",
    "        heads = []\n",
    "        for i in range(1): ### TODO ### CHOOSE THE # OF HEADS YOU WANT\n",
    "            heads.append(self_attention(encoder_dim,dim,dropout)) ### TODO ### ADD SELF_ATTENTION LAYERS TO HEADS\n",
    "        \n",
    "        self.heads = nn.ModuleList(heads)\n",
    "        \n",
    "        self.linear = nn.Linear(dim,encoder_dim) ### TODO ###\n",
    "    \n",
    "    \n",
    "    def forward(self,x,mask=None):\n",
    "        headoutputs = [layer(x,mask) for layer in self.heads]\n",
    "        headoutputs = torch.cat(headoutputs,dim=2)\n",
    "        return self.linear(headoutputs)\n",
    "    \n",
    "class encoder(nn.Module):\n",
    "    def __init__(self,dim,enc_dim,vocab_size,dropout=.1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.enc_dim = enc_dim\n",
    "        self.attention = multi_attention(dim,enc_dim,dropout)\n",
    "        self.norm1 = nn.LayerNorm(enc_dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(enc_dim,enc_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.residual = nn.Linear(dim,enc_dim)\n",
    "        self.norm2 = nn.LayerNorm(enc_dim)\n",
    "    \n",
    "    def forward(self,x,mask):\n",
    "        z = self.attention(x,mask)\n",
    "        if self.dim != self.enc_dim:\n",
    "            x = self.residual(x)\n",
    "        z = self.norm1(x+z)\n",
    "        z2 = self.linear(z)\n",
    "        return self.norm2(z+z2)\n",
    "     \n",
    "    \n",
    "class decoder(nn.Module):\n",
    "    def __init__(self,dim,input_size,vocab_size,dropout=.1):\n",
    "        super().__init__()\n",
    "        self.attention = multi_attention(input_size,dim,dropout)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.EDattention = encdec_attention(dim,dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(dim,dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.norm3 = nn.LayerNorm(dim)\n",
    "    \n",
    "    def forward(self,x,k,v,src,trg):\n",
    "        z = self.attention(x,trg)\n",
    "        z = self.norm1(z+x)\n",
    "        z2 = self.EDattention(z,k,v,src)\n",
    "        z2 = self.norm2(z2+z)\n",
    "        z3 = self.linear(z2)\n",
    "        return self.norm3(z3+z2)\n",
    "    \n",
    "class transformer(nn.Module):\n",
    "    def __init__(self,dim,encoder_dim,enc_vocab_size,dec_vocab_size,input_size):\n",
    "        super().__init__()\n",
    "        self.embedding1 = nn.Embedding(enc_vocab_size,dim)\n",
    "        self.embedding2 = nn.Embedding(dec_vocab_size,encoder_dim)\n",
    "        \n",
    "        self.pe1 = PositionalEncoder(dim,enmax)\n",
    "        self.pe2 = PositionalEncoder(encoder_dim,frmax)\n",
    "        self.encoders = []\n",
    "    \n",
    "        self.encoders.append(encoder(dim,encoder_dim,enc_vocab_size))   ### FEEL FREE TO ADD OR REMOVE ENCODERS\n",
    "        self.encoders = nn.ModuleList(self.encoders)\n",
    "        \n",
    "        self.decoders = []\n",
    "        self.decoders.append(decoder(encoder_dim,encoder_dim,dec_vocab_size)) ### FEEL FREE TO ADD OR REMOVE ENCODERS\n",
    "        self.decoders = nn.ModuleList(self.decoders)\n",
    "        \n",
    "        self.final = nn.Sequential(\n",
    "            nn.Linear(encoder_dim,dec_vocab_size),\n",
    "            nn.LogSoftmax(2)\n",
    "        )\n",
    "        \n",
    "        self.k = nn.Linear(encoder_dim,encoder_dim)\n",
    "        self.v = nn.Linear(encoder_dim,encoder_dim)\n",
    "        \n",
    "    def create_dec_KV(self,z):\n",
    "        K = self.k(z)\n",
    "        V = self.v(z)\n",
    "        return K,V\n",
    "    \n",
    "    def encode(self,x,src):\n",
    "        x = self.embedding1(x)\n",
    "        x = self.pe1(x)\n",
    "        for layer in self.encoders:\n",
    "            x = layer(x,src)\n",
    "        return x\n",
    "    \n",
    "    def decode(self,y,K,V,src,trg):\n",
    "        y = self.embedding2(y)\n",
    "        y = self.pe2(y)\n",
    "        for layer in self.decoders:\n",
    "            y = layer(y,K,V,src,trg)\n",
    "        return self.final(y)\n",
    "        \n",
    "    \n",
    "    def forward(self,x,y,src,trg):\n",
    "        x = self.encode(x,src)\n",
    "        K,V = self.create_dec_KV(x)\n",
    "        y = self.decode(y,K,V,src,trg)\n",
    "        \n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformer(enmax,enmax,len(envocab),len(frvocab),frmax)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01,weight_decay=.00001)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,factor=.1,patience=10,threshold=1,min_lr=.0001)\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  loss: 128.2547265291214\n",
      "\tOutput:  je es <eos> <eos> <eos> <eos>\n",
      "\tTarget:  tu as fais de ton mieux\n",
      "Epoch: 2  loss: 55.19529792666435\n",
      "\tOutput:  vous prie <eos> vous\n",
      "\tTarget:  te souvienstu de nous\n",
      "Epoch: 3  loss: 43.509831219911575\n",
      "\tOutput:  vous avez <eos> si la t\n",
      "\tTarget:  vous n'avez pas de c ur\n",
      "Epoch: 4  loss: 34.64946962893009\n",
      "\tOutput:  il <eos> l <eos> tom\n",
      "\tTarget:  emporter ou consommer sur place\n",
      "Epoch: 5  loss: 29.229242846369743\n",
      "\tOutput:  tesvous s s <eos>\n",
      "\tTarget:  en tesvous s rs\n",
      "Epoch: 6  loss: 26.12924526631832\n",
      "\tOutput:  il estil <eos>\n",
      "\tTarget:  quiconque m'atil entendu\n",
      "Epoch: 7  loss: 24.280047595500946\n",
      "\tOutput:  tu folle\n",
      "\tTarget:  estu inscrite\n",
      "Epoch: 8  loss: 23.19296481460333\n",
      "\tOutput:  garde dehors une dehors <eos> rieur\n",
      "\tTarget:  un garde se trouve l'ext rieur\n",
      "Epoch: 9  loss: 22.176835350692272\n",
      "\tOutput:  tu tes tr s timide\n",
      "\tTarget:  vous tes tr s craintif\n",
      "Epoch: 10  loss: 21.575549334287643\n",
      "\tOutput:  je avezvous dit <eos>\n",
      "\tTarget:  en aije trop dit\n",
      "Epoch: 11  loss: 20.865349039435387\n",
      "\tOutput:  je comme il te de\n",
      "\tTarget:  faites comme il vous dit\n",
      "Epoch: 12  loss: 15.200596734881401\n",
      "\tOutput:  tu tes d gage t\n",
      "\tTarget:  vous tes d go tantes\n",
      "Epoch: 13  loss: 13.00585587322712\n",
      "\tOutput:  vous vous <eos> num parie\n",
      "\tTarget:  c'est toi le ma tre\n",
      "Epoch: 14  loss: 12.051528610289097\n",
      "\tOutput:  vous tes trop tes\n",
      "\tTarget:  vous tes trop maigrichonnes\n",
      "Epoch: 15  loss: 11.419526517391205\n",
      "\tOutput:  ne me ne <eos> <eos> pr\n",
      "\tTarget:  ne vous approchez pas de moi\n",
      "Epoch: 16  loss: 10.947933707386255\n",
      "\tOutput:  ne n'est est facile <eos> <eos> facile les\n",
      "\tTarget:  ce qui est facilement gagn est facilement perdu\n",
      "Epoch: 17  loss: 10.59636365994811\n",
      "\tOutput:  tu ne pouvez pas que <eos> <eos> aller <eos> vous\n",
      "\tTarget:  vous ne pouvez pas simplement vous en aller comme a\n",
      "Epoch: 18  loss: 10.239495571702719\n",
      "\tOutput:  tu en le visage\n",
      "\tTarget:  vous aimez le tennis\n",
      "Epoch: 19  loss: 9.992583774030209\n",
      "\tOutput:  ne le pas <eos> pouse\n",
      "\tTarget:  ne dites rien mon pouse\n",
      "Epoch: 20  loss: 9.790098670870066\n",
      "\tOutput:  qui veut un <eos>\n",
      "\tTarget:  qui veut une boisson\n",
      "Epoch: 21  loss: 9.546897411346436\n",
      "\tOutput:  tu es moiti raison\n",
      "\tTarget:  tu as moiti raison\n",
      "Epoch: 22  loss: 9.4112639836967\n",
      "\tOutput:  astu trouv un travail\n",
      "\tTarget:  astu trouv du travail\n",
      "Epoch: 23  loss: 8.511174138635397\n",
      "\tOutput:  tu devez d en tendre\n",
      "\tTarget:  vous devez vous d tendre\n",
      "Epoch: 24  loss: 8.412147380411625\n",
      "\tOutput:  vous es tr s dr\n",
      "\tTarget:  tu es tr s marrante\n",
      "Epoch: 25  loss: 8.319661010056734\n",
      "\tOutput:  aije dit <eos> quoi\n",
      "\tTarget:  l'aije dit de travers\n",
      "Epoch: 26  loss: 8.330211535096169\n",
      "\tOutput:  estu d ts tarienne\n",
      "\tTarget:  estu v g tarien\n",
      "Epoch: 27  loss: 8.264009837061167\n",
      "\tOutput:  estu v ts tarienne\n",
      "\tTarget:  tesvous v g tarien\n",
      "Epoch: 28  loss: 8.200580459088087\n",
      "\tOutput:  vous tu attention\n",
      "\tTarget:  cela t'atil plu\n",
      "Epoch: 29  loss: 8.116421423852444\n",
      "\tOutput:  tom sera <eos> t\n",
      "\tTarget:  tom seratil pr t\n",
      "Epoch: 30  loss: 8.127951752394438\n",
      "\tOutput:  puisje tre des tien\n",
      "\tTarget:  puisje emprunter le tien\n",
      "Epoch: 31  loss: 8.148679684847593\n",
      "\tOutput:  tesvous tesvous s <eos>\n",
      "\tTarget:  en tesvous s re\n",
      "Epoch: 32  loss: 8.077305905520916\n",
      "\tOutput:  qui es <eos> fils <eos> ton\n",
      "\tTarget:  tu es le fils de qui\n",
      "Epoch: 33  loss: 8.076584469527006\n",
      "\tOutput:  peuxtu le confiance\n",
      "\tTarget:  arrivezvous le croire\n",
      "Epoch: 34  loss: 7.981008026748896\n",
      "\tOutput:  tu avez tous tom tom\n",
      "\tTarget:  vous avez bien fait tom\n",
      "Epoch: 35  loss: 7.9755578972399235\n",
      "\tOutput:  tesvous catholique\n",
      "\tTarget:  tesvous inscrits\n",
      "Epoch: 36  loss: 7.97080697491765\n",
      "\tOutput:  ne me d <eos> <eos>\n",
      "\tTarget:  ne me remercie pas maintenant\n",
      "Epoch: 37  loss: 7.948477812111378\n",
      "\tOutput:  l'avezvous crit a\n",
      "\tTarget:  astu crit a\n",
      "Epoch: 38  loss: 7.949617117643356\n",
      "\tOutput:  avezvous avezvous une avion\n",
      "\tTarget:  en avezvous un exemplaire\n",
      "Epoch: 39  loss: 7.905072811990976\n",
      "\tOutput:  saistu lire <eos> musique\n",
      "\tTarget:  savezvous lire la musique\n",
      "Epoch: 40  loss: 7.835856027901173\n",
      "\tOutput:  ne me pousse pas <eos> moi\n",
      "\tTarget:  ne me traite pas avec condescendance\n",
      "Epoch: 41  loss: 7.784568190574646\n",
      "\tOutput:  une peu <eos> enfant\n",
      "\tTarget:  un enfant est manquant\n",
      "Epoch: 42  loss: 7.742980353534222\n",
      "\tOutput:  estu tais ress <eos>\n",
      "\tTarget:  tesvous int ress e\n",
      "Epoch: 43  loss: 7.809345077723265\n",
      "\tOutput:  joignezvous me moi\n",
      "\tTarget:  viens avec moi\n",
      "Epoch: 44  loss: 7.7266651019454\n",
      "\tOutput:  vous n' tes pas normale\n",
      "\tTarget:  vous n' tes pas normales\n",
      "Epoch: 45  loss: 7.7142903953790665\n",
      "\tOutput:  tu tes fort nerv col re\n",
      "\tTarget:  vous tes fort en col re\n",
      "Epoch: 46  loss: 7.757256388664246\n",
      "\tOutput:  ne faites pas un bruit\n",
      "\tTarget:  ne fais pas un bruit\n",
      "Epoch: 47  loss: 7.640138894319534\n",
      "\tOutput:  vous voulez le <eos> le monde\n",
      "\tTarget:  vous aimez bien tout le monde\n",
      "Epoch: 48  loss: 7.6592558436095715\n",
      "\tOutput:  tout le monde est fait\n",
      "\tTarget:  tout le monde a peur\n",
      "Epoch: 49  loss: 7.59150705114007\n",
      "\tOutput:  tu ne pouvez pas admettre ma arme p\n",
      "\tTarget:  vous ne pouvez pas admettre votre d faite\n",
      "Epoch: 50  loss: 7.564492046833038\n",
      "\tOutput:  tout le monde est tout\n",
      "\tTarget:  tout le monde est choqu\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for j,(context, target) in enumerate(trainloader):\n",
    "        trg_input = target[:,:-1]\n",
    "        outmask = target[:,1:]!= fr_to_ix['<pad>']\n",
    "        targets = target[:,1:].contiguous().view(-1)\n",
    "        src,trg = mask(context,trg_input)\n",
    "        output = model(context,trg_input,src,trg)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output.view(output.shape[0]*output.shape[1],-1),targets)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "    scheduler.step(total_loss)\n",
    "    print('Epoch:', i+1,' loss:', total_loss)\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    preds = []\n",
    "    targetlist = []\n",
    "    for j,(context, target) in enumerate(valloader):\n",
    "            trg_input = target[:,:-1]\n",
    "            targets = target.contiguous().view(-1)\n",
    "            targetlist.append(targets)\n",
    "            src,trg = mask(context,trg_input)\n",
    "            output = model(context,trg_input,src,trg)\n",
    "            pred = F.softmax(output,2).argmax(2)\n",
    "            preds.append(pred)\n",
    "            break\n",
    "    compareoutput(preds,targetlist,loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test your  multi-head transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comment vastu'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'how are you'\n",
    "translate(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QUESTION\n",
    "\n",
    "#### 1) Was the runtime of your multi-head transformer noticably longer than the single head one? What about the speed the loss decreased? If you had the time and resources to train it to a good spot, how did the translation quality compare to the single-headed transformer?\n",
    "\n",
    "#### 2)Try adding encoders and decoders to one of your transformers. Does having the extra layers improve performance? How does it affect runtime?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) No, the runtime was identical, but there was a substatially large loss gradient with respect to epoch. The multi-headed transformer seems superior. \n",
    "\n",
    "2) Extra econder/decoders did not seem to improve performance, and runtime differences seemed neglibible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
